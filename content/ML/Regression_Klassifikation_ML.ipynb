{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9880fe",
   "metadata": {},
   "source": [
    "# Machine Klassifikation und Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73722808",
   "metadata": {},
   "source": [
    "## TEIL A: Klassifikation von Iris Blumen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b22c9",
   "metadata": {},
   "source": [
    "In dieser Aufgabe sehen wir uns das Iris Dataset an. Hierbei geht es darum die Blumen anhand ihrer Blütenblätter- (Petal) und Kelchblättermasse (Sepal) zu klassifizieren.\n",
    "\n",
    "Wir importieren zuerst einmal einige Libraries die wir nutzen möchten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d087d",
   "metadata": {},
   "source": [
    "Nun laden wir das Iris Dataset, welches bereits in der Scikit-Learn Library angeboten wird. Wir speichern es auch in einem DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a7b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#create pandas dataframe from iris\n",
    "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df_iris['target'] = iris.target\n",
    "df_iris['target_names'] = df_iris['target'].apply(lambda x: iris.target_names[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd009e4",
   "metadata": {},
   "source": [
    "Eine Aufgabe, die von Machine Learning übernommen wird, ist die Klassifizierung. \n",
    "\n",
    "Dabei ist es die Aufgabe ein **Data Sample** (z.B. eine Katze oder einen Hund) aufgrund von bestimmten **Features** (z.B. Anzahl Streifen, Grösse) einer bestimmten Kategorie auch **Klasse** genannt zuzuweisen. Die einzelnen Data Samples sind jeweils mit einem **Label** gekennzeichnet, zu welcher Klasse sie gehören."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d8acf",
   "metadata": {},
   "source": [
    "### Aufgabe 1\n",
    "Zeigen Sie nun das Pandas Dataframe mit den Daten an.\n",
    "\n",
    "**Fragen**\n",
    "\n",
    "Was sind die Features die wir in diesem Beispiel nutzen?\n",
    ">sepal width, sepal length, petal width, petal length, \n",
    "\n",
    "Was beinhaltet die Spalte target bzw. target_names?\n",
    ">Dies sind die Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8406671",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d3629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a16ef30a",
   "metadata": {},
   "source": [
    "### Aufgabe 2\n",
    "Wir zeigen nun die paarweise Kombination von den Features an.\n",
    "\n",
    "**Frage**\n",
    "\n",
    "Wenn Sie die Blumen von Auge anhand eines der Diagramme unterscheiden müssten.\n",
    "Welche Feature Kombination würde sich gut eignen? Begründen Sie.\n",
    ">Zum Beispiel petal width und petal length, da dort die Klassen relativ gut separiert sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1728b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the feature spaces with two features each\n",
    "pairplt = sns.pairplot(df_iris.drop('target', axis=1), hue='target_names', height=2, palette='tab10')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff3350c",
   "metadata": {},
   "source": [
    "### Aufgabe 3\n",
    "In diesem Fall möchten wir nun die Blumen in die Klassen setosa (0), versicolor (1) und verginica (2) einteilen. Die Daten haben alle bereits die sogenannten **Labels** zugewiesen. Diese werden manchmal auch target genannt und sind in den Daten in den Spalten target und target_names vorhanden.\n",
    "\n",
    "Wenn wir ein oder mehrere Features in einem mathematischen Raum kombinieren, entsteht ein sogennanter **Feature Space** (Merkmalsraum).\n",
    "Ein solcher Feature Space für die *petal width* und *petal length* sieht wie folgt aus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='petal length (cm)', y='petal width (cm)',\n",
    "                hue='target_names', data=df_iris.drop('target', axis=1), palette='tab10')\n",
    "\n",
    "# Placing Legend outside the Figure\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60405aaa",
   "metadata": {},
   "source": [
    "\n",
    "**Fragen**\n",
    "\n",
    "Wie viele Dimensionen hat der Feature Space im obigen Beispiel?\n",
    "> Zwei Dimensionen, petal length und petal width\n",
    "\n",
    "Wie viele Dimensionen können wir in einem Feature Space haben?\n",
    "> Theoretisch beliebig viele grösser 0. Jedoch bringt jede Dimension weitere Komplexität zum Problem was sich in längerer Rechenzeit niederschlägt um die optimale Klassifikation zu berechnen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83762628",
   "metadata": {},
   "source": [
    "### Aufgabe 4\n",
    "\n",
    "Ein Machine Learning Model versucht eine oder mehrere **Decision Boundaries** also Entscheidungsgrenzen in den Feature Space zu platzieren, so dass möglichst viele Data Samples korrekt klassifiziert werden.\n",
    "\n",
    "**Frage**\n",
    "\n",
    "Beschreiben Sie wie Sie zwei linearen Decision Boundaries im Feature Space oben platzieren würden um die drei Klassen auseinanderzuhalten.\n",
    "> Dies könnte man zum Beispiel machen, indem man bei petal width bei 0.8cm eine horizontale Decision boundary und bei petal length bei zirka 4.8cm eine senkrechte lineare Decision boundary gelegt wird.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6905907d",
   "metadata": {},
   "source": [
    "### Aufgabe 5\n",
    "\n",
    "Machine Learning Modelle treffen Entscheidungen anhand von Daten, welche Sie zu Beginn der «Kalibrierung» genutzt haben, um zu lernen. Man nennt diese Daten **Trainingsdaten**, da das Modell sozusagen damit trainiert wird. Wenn nun ein Machine Learning Modell getestet wird (heisst: wie gut funktioniert das Modell?), werden ihm neue Daten sogenannte **Testdaten** präsentiert, welche nicht für das Training genutzt wurden. \n",
    "\n",
    "**Fragen**\n",
    "\n",
    "Was ist der Vorteil bei diesem Vorgehen? \n",
    "> Man kann nun prüfen ob das Modell gut generalisiert und somit mit ungesehenen Daten gut umgehen kann.\n",
    "\n",
    "Was könnte ein Nachteil sein?\n",
    "> Man muss genügend Daten haben um ein robustes Training durchführen zu können und dann auch noch zu testen. \n",
    "> Die Testdaten müssen realistisch sein und können nicht einfach nur erfunden sein.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe25c47",
   "metadata": {},
   "source": [
    "### Aufgabe 6 \n",
    "\n",
    "Damit wir Trainings- und Testdaten haben, teilen wir die Daten in Trainings und Testdaten ein. Das gleiche passiert auch mit den targets. Wir nutzen eine fertige Funktion von sklearn dafür.\n",
    "Vielfach werden die Features in einer Variable X gespeichert und die Labels bzw. Targets in einer Variable y.\n",
    "\n",
    "**Fragen**\n",
    "\n",
    "In welchem Verhältnis werden die Daten mit dem Befehl train_test_split unten aufgeteilt?\n",
    "> Die Daten werden im Verhältnis 80% Training und 20% Testing aufgeteilt.\n",
    "\n",
    "Wie viele Data Samples sind nun im Trainings Dataset und wie viele im Testdataset?\n",
    "> 120 im Trainingsdatenset und 30 im Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de847c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_housing_train, X_housing_test, y_housing_train, y_housing_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ausgabe der Anzahl der Trainings- und Testdaten\n",
    "print(f'Anzahl Trainingsdaten: {X_housing_train.shape[0]}')\n",
    "print(f'Anzahl Testdaten: {X_housing_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a0241",
   "metadata": {},
   "source": [
    "### Aufgabe 7\n",
    "\n",
    "Wir trainieren nun unseren ersten Classifier und testen diesen auch. \n",
    "\n",
    "Vorerst nutzen wir einen fixfertigen Classifier von Sklearn den wir instanziieren müssen und dann \"fitten\". Hinter der fit Funktion versteckt sich ein Trainingsprozess in dem das Modell von den Trainingsdaten lernt.\n",
    "Vervollständige die Befehle anhand der Kommentare.\n",
    "\n",
    "**Bemerkung:** Das Modell wurde extra so konfiguriert, dass nicht eine 100% Genauigkeit resultiert, damit wir die Genauigkeit auswerten können. Deshalb erscheint auch eine **ConvergenceWarning**, diese können Sie **ignorieren**.\n",
    "\n",
    "Wir evaluieren das Modell mittels der Accuracy. Diese wird berechnet indem die Anzahl korrekt klassifizierten Data Samples geteilt durch die Anzahl aller Data Samples gerechnet wird:\n",
    "\n",
    "  $ Accuracy = \\frac{\\text{Anzahl korrekt Klassifizierte Data Samples}}{\\text{Alle Data Samples}} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(10,), max_iter=500, random_state=42)\n",
    "\n",
    "# Trainiere das Modell mit den Trainignsdaten als erstes Argument und den zugehörigen Labels als zweites Argument\n",
    "mlp.fit(X_housing_train, y_housing_train)\n",
    "\n",
    "# Vorhersage der Labels für die Testdaten\n",
    "y_housing_pred = mlp.predict(X_housing_test)\n",
    "\n",
    "# Berechne die Genauigkeit des Modells\n",
    "# Dies können wir berechnen indem wir zählen, wie viele der vorhergesagten Labels mit den tatsächlichen Labels übereinstimmen und dies durch die Gesamtanzahl der Testdaten teilen\n",
    "accuracy = np.sum(y_housing_pred == y_housing_test) / len(y_housing_test)\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9171e4",
   "metadata": {},
   "source": [
    "### Aufgabe 8\n",
    "\n",
    "Wir schauen uns nun noch genauer an, welche Klassen am meisten verwechselt wurden. Dazu werden wir eine Konfusionsmatrix nutzen. Dabei werden die tatsächlichen Labels den vorhergesagten Labels gegenübergestellt und aufgezeichnet, wie oft jede Kombination vorkommt.\n",
    "\n",
    "\n",
    "**Frage**\n",
    "Betrachten Sie nun nochmals die paarweisen Feature Spaces aus Aufgabe 2 betrachten.\n",
    "Welche zwei Klassen werden wohl am meisten verwechselt?\n",
    "> Wahrscheinlich die Klassen virginica und versicolor, da diese in den meisten Featurespaces überlappen.\n",
    "\n",
    "Lassen Sie nun die Zelle unten laufen und interpretieren Sie die Konfusionsmatrix. Stimmte ihre Vermutung?\n",
    "> Ja.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir zeigen nun eine Konfusionsmatrix an, um die Leistung des Modells zu visualisieren. \n",
    "\n",
    "conf_matrix = ConfusionMatrixDisplay.from_predictions(y_housing_test, y_housing_pred, display_labels=iris.target_names)\n",
    "conf_matrix.ax_.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac9a04",
   "metadata": {},
   "source": [
    "## Kontrollfragen Klassifikation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147ef13",
   "metadata": {},
   "source": [
    "**Kontrollfrage 1**\n",
    "\n",
    "Ein Machine-Learning Modell klassifiziert die Schüler*innen ob sie mit dem Velo, den ÖV oder zu Fuss zur Schule kommen. \n",
    "In den Daten wird pro Schüler*in und Tag erfasst, wie weit der Schulweg ist, wie lange die Reisedauer war, ob der/die Schüler*in ein Velo besitzt und wie weit die nächste ÖV-Haltestelle vom Wohnort entfernt ist.\n",
    "\n",
    "Was sind in diesem Beispiel\n",
    "\n",
    "    - Features?\n",
    "    - Klassen?\n",
    "    - Data Samples?\n",
    "    \n",
    "Was wäre ein Beispiel für eine Ein und Ausgabe des Modells?\n",
    "\n",
    ">Klassen: Velo, ÖV, zu Fuss\n",
    ">\n",
    ">Features: Schulweg_Länge, Reisedauer, Velo_Ja_Nein, Distanz_ÖV\n",
    ">\n",
    ">Data-Samples: Einzelne Datensätze pro Schüler*in mit den Angaben zu den Features\n",
    ">\n",
    ">Ein Beispiel wäre\n",
    ">Eingabe: \n",
    ">    Schulweg: 5\n",
    ">    Reisedauer: 25\n",
    ">    Velo: false oder 0\n",
    ">    ÖV_Dist: 0.2\n",
    ">Ausgabe:\n",
    ">    ÖV (Klasse 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Kontrollfrage 2**\n",
    "\n",
    "Beschreiben Sie das Vorgehen, um ein Klassifikationsmodell zu evaluieren und dabei die Accuracy zu berechnen.\n",
    "\n",
    "> Für die Testdaten sagt das Modell die Klassen voraus. Danach wird berechnet wie viele der Voraussagen korrekt waren und diese Anzahl wird durch die gesamte Anzahl an Test Data Samples geteilt. Somit hat man einen Prozentsatz, wie viele der Voraussagen korrekt waren.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64193641",
   "metadata": {},
   "source": [
    "## TEIL B: Regression mit Hauspreisberechnung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e0ed32",
   "metadata": {},
   "source": [
    "In diesem Dataset wurden verschiedene Eigenschaften von Liegenschaften erfasst. Dabei soll nun von den Eigenschaften auf den Hauspreis geschlossen werden. Der Hauspreis ist somit die **Zielvariable** oder engl. *Target*, ähnlich dem Label in der Klassifikation.\n",
    "\n",
    "Die Berechnungen des Hauspreises, werden wir mit einem Regressionsmodell machen.\n",
    "\n",
    "Das Dataset das wir benutzten, ist das California Housing Dataset:\n",
    "https://media.geeksforgeeks.org/wp-content/uploads/20240522145850/housing%5B1%5D.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if file housing.csv exists, if not download it\n",
    "if not os.path.exists(\"housing.csv\"):\n",
    "    #load housing dataset from URL and save file locally\n",
    "    url = \"https://media.geeksforgeeks.org/wp-content/uploads/20240522145850/housing%5B1%5D.csv\"\n",
    "    response = requests.get(url)\n",
    "    with open(\"housing.csv\", \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "# load csv into a pandas dataframe\n",
    "df_housing = pd.read_csv(\"housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e65082",
   "metadata": {},
   "source": [
    "### Aufgabe 9\n",
    "\n",
    "Sie haben sich sicherlich die Features im Dataframe angeschaut. Machine Learning Modelle benötigen die Daten als Zahlen um diese im Features Space abbilden zu können. Jedoch haben wir mit ocean_proximity ein Feature das Kategorische Daten enthält. Diese können wir mit dem sogenannten One-Hot-Encoding in einen mathematischen Raum übertragen. Dies geschieht indem wir für jede Kategorie eine neue Dimension anlegen und dort eine 1 vermerken wenn die Kategorie zutrifft und bei allen anderen eine 0. Wir nutzen dazu den One-Hot-Encoder von Scikit-learn.\n",
    "\n",
    "Zusätzlich entfernen wir noch alle Data Samples die leere Werte haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030101bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode ocean_proximity with one-hot-encoding\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "ohe.fit(df_housing[['ocean_proximity']])\n",
    "df_housing_encoded = pd.concat([df_housing, pd.DataFrame(ohe.transform(df_housing[['ocean_proximity']]), columns=ohe.get_feature_names_out(['ocean_proximity']))], axis=1)\n",
    "df_housing_encoded.drop('ocean_proximity', axis=1, inplace=True)\n",
    "\n",
    "# Remove rows with missing values, as these cannot be used for training the model\n",
    "df_housing_encoded.dropna(inplace=True)\n",
    "\n",
    "df_housing_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49801d8",
   "metadata": {},
   "source": [
    "### Aufgabe 10\n",
    "\n",
    "Wir möchten nun noch die Daten normalisieren. Dies hilft einigen Modellen zum Beispiel künstlichen Neuronalen Netzwerken schneller zu optimieren und zu lernen.\n",
    "\n",
    "Wir wenden die min-max-Skalierung an. Das heisst alle Features haben danach einen minimalen Wert von 0 und einen maximalen Wert von 1.\n",
    "\n",
    "Wie könnten Sie dies berrechnen? Vervollständige danach den Code unten.\n",
    "\n",
    "<details>\n",
    "<summary><b>Tipp 1:</b> Klicke hier für den ersten Tipp.</summary>\n",
    "\n",
    "Wie nutzen Sie das Minimum eines Features und das Maximum damit nachher alle Werte eines Features zwischen (inklusive) 0 und 1 sind?\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>Lösung:</b> Klicken Sie hier um die Formel anzuzeigen.</summary>\n",
    "\n",
    "$scaled\\_value = \\frac{value-min}{max - min}$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Normalize numerical features with min max scaling\n",
    "\n",
    "# Identify numerical features\n",
    "numerical_features = df_housing_encoded.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Apply min-max scaling\n",
    "for feature in numerical_features:\n",
    "    min_value = df_housing_encoded[feature].min()\n",
    "    max_value = df_housing_encoded[feature].max()\n",
    "    df_housing_encoded[feature] = (df_housing_encoded[feature] - min_value) / (max_value - min_value)\n",
    "\n",
    "df_housing_encoded\n",
    "\n",
    "# prüfen ob die numerischen Features korrekt normalisiert wurden\n",
    "assert (df_housing_encoded[numerical_features].min().min() >= 0) and (df_housing_encoded[numerical_features].max().max() <= 1), \"Die numerischen Features wurden nicht korrekt normalisiert.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a48e1e",
   "metadata": {},
   "source": [
    "### Aufgabe 11\n",
    "\n",
    "Unterteilen Sie das Dataset in ein Trainings und Testteil wie im vorherigen Abschnitt bereits gemacht.\n",
    "Nutzen Sie auch einen Train/Test Split von 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc6c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unterteile das Dataset in Trainigns- und Testdaten. Die Spalte 'median_house_value' ist die Zielvariable, die wir vorhersagen möchten. Deshalb wird sie von den Features getrennt.\n",
    "\n",
    "X_housing_train, X_housing_test, y_housing_train, y_housing_test = train_test_split(df_housing_encoded.drop('median_house_value', axis=1), df_housing_encoded['median_house_value'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe772d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tests um zu prüfen ob die Aufteilung korrekt ist\n",
    "assert X_housing_train.shape[0] == 16346, f\"Erwartete Anzahl Trainingsdaten: 16346, aktuell sind es: {X_housing_train.shape[0]}\"\n",
    "assert X_housing_test.shape[0] == 4087 , f\"Erwartete Anzahl Testdaten: 4087, aktuell sind es: {X_housing_test.shape[0]}\"\n",
    "\n",
    "# Prüfe ob median_house_value aus den Features entfernt wurde\n",
    "assert 'median_house_value' not in X_housing_train.columns, \"median_house_value wurde nicht aus den Features entfernt.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0690ff11",
   "metadata": {},
   "source": [
    "### Aufgabe 12\n",
    "\n",
    "1. Nutzen Sie die MLPRegressor Klasse um ein Modell zu instantieren. Die Klasse wurde bereits am Anfang importiert. Sie können die gleichen Parameter verwenden wie in Aufgabe 7 beim MLPClassifier.\n",
    "2. Trainieren Sie nun das Modell mit dem Aufruf der fit(Trainingsdaten, Targets) Methode.\n",
    "\n",
    "Optional: Weitere Infos zur MLPRegressor Klasse: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45141773",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(10,), max_iter=800, random_state=42)\n",
    "\n",
    "# Trainiere das Modell mit den Trainignsdaten als erstes Argument und den zugehörigen Labels als zweites Argument\n",
    "mlp_regressor.fit(X_housing_train, y_housing_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05781762",
   "metadata": {},
   "source": [
    "### Aufgabe 13\n",
    "\n",
    "Evaluieren Sie nun ihr Modell mit den Testdaten. Dieses Mal können wir aber nicht die Accuracy nutzen, da diese nur für Klassifikationen geeignet ist.\n",
    "Wir nutzen stattdessen den Mean-Squared-Error. Dieser wird wie folgt berechnet:\n",
    "\n",
    "- $y$: Echtes Label\n",
    "- $\\hat{y}$: Voraussage des Modells\n",
    "\n",
    "$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "In Prosa geschieht hier folgendes:\n",
    "Für jedes Data Samples im Testdatenset wird das echte Label minus der Voraussage gerechnet. Dieses Ergebnis wird quadriert. Danach wird die Summe über alle diese quadrierten \"Fehler\" berechnet und geteilt durch die Anzahl Samples gerechnet. Somit der Mittelwert des quadrierten Fehlers.\n",
    "\n",
    "Vervollständigen Sie den Code um den MSE zu berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55863e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set with the MSE (Mean Squared Error) metric\n",
    "\n",
    "y_housing_pred = mlp_regressor.predict(X_housing_test)\n",
    "\n",
    "mse_test = np.sum((y_housing_pred - y_housing_test) ** 2) / len(y_housing_test)\n",
    "print(f'Mean Squared Error on Test Set: {mse_test:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6102df",
   "metadata": {},
   "source": [
    "### Aufgabe 14\n",
    "Führen Sie die Code-Zelle unten aus. Dabei wird für ein Datasample aus dem Test Dataset der Hauspreis berechnet.\n",
    "\n",
    "Was fällt Ihnen bei dieser Vorhersage auf?\n",
    "\n",
    "> Wir haben die Daten, also auch die Zielvariable mit dem Min-Max-Normalisierung skaliert. Dies müssen wir nun bei einer Voraussage wieder rückgängig machen. Eine Lösung ist das Minimum und Maximum der Hauspreise aus den Daten zu speichern und die Skalierung umzukehren: $ value = \\text{scaled\\_value} * (max - min) + min$\n",
    "\n",
    "Weshalb ist die Vorhersage in dieser Grössenordnung und wie könnten Sie dieses Problem lösen?\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "<summary><b>Tipp 1:</b> Klicke hier für den einen Tipp.</summary>\n",
    "\n",
    "Wir haben auch den House Value mit Min Max Normalisierung skaliert. Wie könnte man dies nun zu einem korrekten Hauswert zurückrechnen?\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1488a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction for a single test instance\n",
    "\n",
    "y_housing_pred_single = mlp_regressor.predict(X_housing_test[:1])\n",
    "print(f'Der berechnete Hauswert beträgt: {y_housing_pred_single[0]:.2f}')\n",
    "\n",
    "y_pred_scaled = y_housing_pred_single * (df_housing['median_house_value'].max() - df_housing['median_house_value'].min()) + df_housing['median_house_value'].min()\n",
    "print(f'Der berechnete Hauswert im Originalmaßstab beträgt: {y_pred_scaled[0]:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54679056",
   "metadata": {},
   "source": [
    "### Aufgabe 15\n",
    "\n",
    "Wir zeigen nun in einem Scatter Plot noch einige zufällige Datenpunkte an, wobei wir vergleichen möchten was der echte Hauspreis ist und was unser Modell berechnet hat.\n",
    "Lassen Sie die nächste Code Zelle laufen und beantworten Sie die folgende Frage.\n",
    "\n",
    "**Frage**\n",
    "Woran erkennt man einen kleinen Fehler des Modells und wie einen grossen?\n",
    "> Indem man den Abstand der beiden Punkte betrachtet. Ein grosser Abstand entspricht einem grossen Fehler und ein kleiner Abstand einen kleinen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4bc3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotte die Vorhersagen des Modells gegen die tatsächlichen Werte nutze aber nur 50 zufällige Datenpunkte und zeichne den Fehler als Linie ein\n",
    "\n",
    "random_indices = np.random.choice(len(y_housing_test), size=50, replace=False)\n",
    "y_housing_pred_sampled = y_housing_pred[random_indices]\n",
    "y_housing_test_sampled = y_housing_test.iloc[random_indices]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.scatter(range(len(y_housing_pred_sampled)), y_housing_pred_sampled, color='red', label='Berechnete Werte')\n",
    "plt.scatter(range(len(y_housing_test_sampled)), y_housing_test_sampled, color='blue', label='Tatsächliche Werte')\n",
    "for i in range(len(y_housing_pred_sampled)):\n",
    "    plt.plot([i, i], [y_housing_pred_sampled[i], y_housing_test_sampled.iloc[i]], color='gray', linestyle='--', linewidth=0.5)\n",
    "plt.xlabel('Testdaten Index')\n",
    "plt.ylabel('Median Hauswert (normalisiert)')\n",
    "plt.title('Vorhersagen vs Tatsächliche Werte des Hauswerts')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9126ef",
   "metadata": {},
   "source": [
    "### Zusatzaufgabe: Teste das Training ohne min-max Normalisierung\n",
    "\n",
    "Führe nochmals einen Traingslauf durch ohne, dass die min-max Skalierung genutzt wurde.\n",
    "Beobachte wie lange das Training nun läuft. \n",
    "\n",
    "**Frage**: Was ist schneller? Min-Max normalisierte Daten oder die ursprünglichen Daten?\n",
    "\n",
    ">Das Training benötigt plötzlich mehrere Sekunden anstatt einem Bruchteil einer Sekunde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eaa366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0a10b4c",
   "metadata": {},
   "source": [
    "## Kontrollfragen: Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9718682d",
   "metadata": {},
   "source": [
    "**Kontrollfrage 3**\n",
    "\n",
    "Was ist der Output einer Regression und wie verhält sich dieser im Vergleich zu der Klassifikation?\n",
    "> Die Regression hat einen numerischen und kontinuierlichen Output. Im Vergleich zur Klassifikation welche diskrete Klassen als Ausgabe hat.\n",
    "\n",
    "\n",
    "**Kontrollfrage 4**\n",
    "\n",
    "Welchen Vorteil hat die Normalisierung der numerischen Features gebracht? Wie lautete die Formel?\n",
    "> Der Vorteil war, dass das Training des Modells viel schneller war.\n",
    ">\n",
    "> $scaled\\_value = \\frac{value-min}{max - min}$ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "praktikum_ef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
